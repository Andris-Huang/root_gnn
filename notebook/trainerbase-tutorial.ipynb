{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/global/u2/a/ading/root_gnn/notebook\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup python Environment\n",
    "\n",
    "1) create an isolated python environment namely `gnn` via [conda](https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#creating-an-environment-with-commands). \n",
    "\n",
    "\\[Optional\\] Create a configuration file for `conda`: `~/.condarc`, \n",
    "and specify the location of envrionments that will house python modules. \n",
    "This directory will grow very quickly. I suggest to use a project directory.\n",
    "```json\n",
    "envs_dirs:\n",
    "  - /global/cfs/cdirs/atlas/xju/conda/envs\n",
    "report_errors: true\n",
    "```\n",
    "\n",
    "1.1) Following commands is to install an environment named `gnn`. \n",
    "```bash\n",
    "module load python\n",
    "conda create -n gnn python=3.8 ipykernel\n",
    "source $(which conda | sed -e s#bin/conda#bin/activate#)  gnn\n",
    "python -m ipykernel install --user --name gnn --display-name a-Gnn\n",
    "```\n",
    "\n",
    "It will install a kernel file at `~/.local/share/jupyter/kernels/gnn/kernel.json`. \n",
    "\n",
    "1.2) create a `~/.local/share/jupyter/kernels/gnn/setup.sh` with the following contents:\n",
    "```bash\n",
    "#!/bin/bash\n",
    "module load python\n",
    "source $(which conda | sed -e s#bin/conda#bin/activate#)  gnn\n",
    "python -m ipykernel_launcher $@\n",
    "```\n",
    "and make it executable `chmod +x ~/.local/share/jupyter/kernels/gnn/setup.sh`.\n",
    "\n",
    "Get absolute path: `readlink -f ~/.local/share/jupyter/kernels/gnn/setup.sh`.\n",
    "\n",
    "1.3) update the `~/.local/share/jupyter/kernels/gnn/kernel.json` as the following. \n",
    "Note that the path to `setup.sh` should be the absolute path.\n",
    "```json\n",
    "{\n",
    " \"argv\": [\n",
    "  \"/global/u1/x/xju/.local/share/jupyter/kernels/gnn/setup.sh\",\n",
    "  \"-f\",\n",
    "  \"{connection_file}\"\n",
    " ],\n",
    " \"display_name\": \"a-Gnn\",\n",
    " \"language\": \"python\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/global/cfs/cdirs/m3443/usr/ading/conda/envs/gnn/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/global/cfs/cdirs/m3443/usr/ading/conda/envs/gnn/bin/pip\n"
     ]
    }
   ],
   "source": [
    "!which pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.4.1-cp38-cp38-manylinux2010_x86_64.whl (394.4 MB)\n",
      "Requirement already satisfied: wheel~=0.35 in /global/cfs/cdirs/m3443/usr/ading/conda/envs/gnn/lib/python3.8/site-packages (from tensorflow) (0.36.2)\n",
      "Collecting tensorboard~=2.4\n",
      "  Using cached tensorboard-2.4.1-py3-none-any.whl (10.6 MB)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Using cached termcolor-1.1.0-py3-none-any.whl\n",
      "Collecting typing-extensions~=3.7.4\n",
      "  Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting grpcio~=1.32.0\n",
      "  Using cached grpcio-1.32.0-cp38-cp38-manylinux2014_x86_64.whl (3.8 MB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.15.6-cp38-cp38-manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 26.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta~=0.2\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting numpy~=1.19.2\n",
      "  Using cached numpy-1.19.5-cp38-cp38-manylinux2010_x86_64.whl (14.9 MB)\n",
      "Requirement already satisfied: six~=1.15.0 in /global/cfs/cdirs/m3443/usr/ading/conda/envs/gnn/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Collecting tensorflow-estimator<2.5.0,>=2.4.0\n",
      "  Using cached tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "\u001b[K     |████████████████████████████████| 129 kB 98.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gast==0.3.3\n",
      "  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting h5py~=2.10.0\n",
      "  Using cached h5py-2.10.0-cp38-cp38-manylinux1_x86_64.whl (2.9 MB)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting wrapt~=1.12.1\n",
      "  Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.27.1-py2.py3-none-any.whl (136 kB)\n",
      "\u001b[K     |████████████████████████████████| 136 kB 89.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /global/cfs/cdirs/m3443/usr/ading/conda/envs/gnn/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (52.0.0.post20210125)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Using cached Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 20.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Using cached cachetools-4.2.1-py3-none-any.whl (12 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.4-py2.py3-none-any.whl (153 kB)\n",
      "\u001b[K     |████████████████████████████████| 153 kB 80.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna<3,>=2.5\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 16.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting chardet<5,>=3.0.2\n",
      "  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
      "\u001b[K     |████████████████████████████████| 178 kB 97.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /global/cfs/cdirs/m3443/usr/ading/conda/envs/gnn/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Installing collected packages: urllib3, pyasn1, idna, chardet, rsa, requests, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, werkzeug, tensorboard-plugin-wit, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, typing-extensions, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "Successfully installed absl-py-0.12.0 astunparse-1.6.3 cachetools-4.2.1 chardet-4.0.0 flatbuffers-1.12 gast-0.3.3 google-auth-1.27.1 google-auth-oauthlib-0.4.3 google-pasta-0.2.0 grpcio-1.32.0 h5py-2.10.0 idna-2.10 keras-preprocessing-1.1.2 markdown-3.3.4 numpy-1.19.5 oauthlib-3.1.0 opt-einsum-3.3.0 protobuf-3.15.6 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.25.1 requests-oauthlib-1.3.0 rsa-4.7.2 tensorboard-2.4.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.4.1 tensorflow-estimator-2.4.0 termcolor-1.1.0 typing-extensions-3.7.4.3 urllib3-1.26.4 werkzeug-1.0.1 wrapt-1.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the python package [root_gnn](https://github.com/xju2/root_gnn/tree/tf2) using the branch `tf2` therein. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -e .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/global/homes/a/ading/atlas/data/top-tagger/test.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up graphs for training, validation, and testing\n",
    "\n",
    "1. Creating training graphs\n",
    "\n",
    "```bash\n",
    "create_tfrecord /global/homes/x/xju/atlas/data/top-tagger/train.h5 tfrec/train \\\n",
    "  --evts-per-record 100 --max-evts 1000 \\\n",
    "  --type TopTaggerDataset --num-workers 2\n",
    "```\n",
    "\n",
    "\n",
    "2. Creating validating graphs\n",
    "\n",
    "```bash\n",
    "create_tfrecord /global/homes/x/xju/atlas/data/top-tagger/val.h5 tfrec/val \\\n",
    "  --evts-per-record 100 --max-evts 1000 \\\n",
    "  --type TopTaggerDataset --num-workers 2\n",
    "```\n",
    "\n",
    "\n",
    "3. Creating testing graphs\n",
    "\n",
    "```bash\n",
    "create_tfrecord /global/homes/x/xju/atlas/data/top-tagger/test.h5 tfrec/test \\\n",
    "  --evts-per-record 100 --max-evts 1000 \\\n",
    "  --type TopTaggerDataset --num-workers 2\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The main script, train_classifier, can be invoked with the following bash command with default arguments:\n",
    "\n",
    "```bash\n",
    "train_classifier\n",
    "```\n",
    "\n",
    "or with the following arguments specifying I/O and hyperparameters:\n",
    "```bash\n",
    "train_classifier --input-dir tfrec --output-dir trained \\\n",
    "  --batch-size 25 --num-epochs 10 --num-iters 10 --lr 0.002\n",
    "```\n",
    "\n",
    "You can also specify other models and loss functions defined in ```root_gnn/model.py``` and ```root_gnn/losses.py```.\n",
    "\n",
    "Let's examine what ```train_classifier``` is doing under the hood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import functools\n",
    "import six\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "\n",
    "\n",
    "from graph_nets import utils_tf\n",
    "from graph_nets import utils_np\n",
    "import sonnet as snt\n",
    "\n",
    "from root_gnn import model as all_models\n",
    "from root_gnn import losses\n",
    "from root_gnn.src.datasets import graph\n",
    "from root_gnn.utils import load_yaml\n",
    "\n",
    "from root_gnn import trainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = getattr(all_models, \"GlobalClassifierNoEdgeInfo\")()\n",
    "loss_config = \"GlobalLoss,1,1\".split(',')\n",
    "loss_fcn = getattr(losses, loss_config[0])(*[float(x) for x in loss_config[1:]])\n",
    "config = {\n",
    "    \"input_dir\": \"../tfrec\",\n",
    "    \"output_dir\": \"../trained\",\n",
    "    \"batch_size\": 50,\n",
    "    \"num_epochs\": 5,\n",
    "    \"num_iters\": 10,\n",
    "    \"shuffle_size\": 1,\n",
    "    \"model\": model,\n",
    "    \"loss_name\": loss_config[0],\n",
    "    \"loss_fcn\": loss_fcn,\n",
    "    \"lr\": 0.001,\n",
    "    \"metric_mode\": \"clf\",\n",
    "    \"early_stop\": \"auc\",\n",
    "    \"max_attempts\": 1\n",
    "}\n",
    "trnr = trainer.TrainerBase(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```TrainerBase()``` constructor initializes a base trainer object by unpacking the ```config``` dict.\n",
    "\n",
    "Next, the user can call functions for loading training, validation, and testing data. The requirement is that the files to be extracted from```input_dir``` must be of the proper ```.tfrec``` format created by ```create_tfrecord```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, _ = trnr.load_training_data(shuffle=True)\n",
    "val_data, _ = trnr.load_validating_data(shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```train``` function of ```TrainerBase``` performs training given the specified configurations and hyperparameters. The function can be called in two main ways:\n",
    "\n",
    "The first way is the default call, which assumes the model and loss are the same as the configurations, and that the training data is the same as the last call to ```load_training_data```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trnr.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second way to call ```train``` is by specifying either a model, loss, or training data. The training data must be of the format returned as the first tuple value of ```load_training_data```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trnr.train(model, loss_fcn, train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this next part, we will be using TensorBoard with the ```nersc_tensorboard_helper```. At this point, it is recommended to switch the notebook kernel away from \"a-Gnn\" to \"tensorflow-v2.0.0-cpu\" to access the tensorboard helper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nersc_tensorboard_helper\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-e6c9e503ae13da71\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-e6c9e503ae13da71\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 38971;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir /global/homes/a/ading/root_gnn/trained/noedge_fullevts/logs --port 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://jupyter.nersc.gov/user/ading/cori-shared-node-cpu/proxy/38971/\">https://jupyter.nersc.gov/user/ading/cori-shared-node-cpu/proxy/38971/</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nersc_tensorboard_helper.tb_address()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now access the link above to view the TensorBoard for your training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-v2.0.0-cpu",
   "language": "python",
   "name": "tensorflow_intel_2.0.0-py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
