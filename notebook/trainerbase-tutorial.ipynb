{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup python Environment\n",
    "\n",
    "1) create an isolated python environment namely `gnn` via [conda](https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#creating-an-environment-with-commands). \n",
    "\n",
    "\\[Optional\\] Create a configuration file for `conda`: `~/.condarc`, \n",
    "and specify the location of envrionments that will house python modules. \n",
    "This directory will grow very quickly. I suggest to use a project directory.\n",
    "```json\n",
    "envs_dirs:\n",
    "  - /global/cfs/cdirs/atlas/xju/conda/envs\n",
    "report_errors: true\n",
    "```\n",
    "\n",
    "1.1) Following commands is to install an environment named `gnn`. \n",
    "```bash\n",
    "module load python\n",
    "conda create -n gnn python=3.8 ipykernel\n",
    "source $(which conda | sed -e s#bin/conda#bin/activate#)  gnn\n",
    "python -m ipykernel install --user --name gnn --display-name a-Gnn\n",
    "```\n",
    "\n",
    "It will install a kernel file at `~/.local/share/jupyter/kernels/gnn/kernel.json`. \n",
    "\n",
    "1.2) create a `~/.local/share/jupyter/kernels/gnn/setup.sh` with the following contents:\n",
    "```bash\n",
    "#!/bin/bash\n",
    "module load python\n",
    "source $(which conda | sed -e s#bin/conda#bin/activate#)  gnn\n",
    "python -m ipykernel_launcher $@\n",
    "```\n",
    "and make it executable `chmod +x ~/.local/share/jupyter/kernels/gnn/setup.sh`.\n",
    "\n",
    "Get absolute path: `readlink -f ~/.local/share/jupyter/kernels/gnn/setup.sh`.\n",
    "\n",
    "1.3) update the `~/.local/share/jupyter/kernels/gnn/kernel.json` as the following. \n",
    "Note that the path to `setup.sh` should be the absolute path.\n",
    "```json\n",
    "{\n",
    " \"argv\": [\n",
    "  \"/global/u1/x/xju/.local/share/jupyter/kernels/gnn/setup.sh\",\n",
    "  \"-f\",\n",
    "  \"{connection_file}\"\n",
    " ],\n",
    " \"display_name\": \"a-Gnn\",\n",
    " \"language\": \"python\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the python package [root_gnn](https://github.com/xju2/root_gnn/tree/tf2) using the branch `tf2` therein. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -e .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/global/homes/a/ading/atlas/data/top-tagger/test.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up graphs for training, validation, and testing\n",
    "\n",
    "1. Creating training graphs\n",
    "\n",
    "```bash\n",
    "create_tfrecord /global/homes/x/xju/atlas/data/top-tagger/train.h5 tfrec/train \\\n",
    "  --evts-per-record 100 --max-evts 1000 \\\n",
    "  --type TopTaggerDataset --num-workers 2\n",
    "```\n",
    "\n",
    "\n",
    "2. Creating validating graphs\n",
    "\n",
    "```bash\n",
    "create_tfrecord /global/homes/x/xju/atlas/data/top-tagger/val.h5 tfrec/val \\\n",
    "  --evts-per-record 100 --max-evts 1000 \\\n",
    "  --type TopTaggerDataset --num-workers 2\n",
    "```\n",
    "\n",
    "\n",
    "3. Creating testing graphs\n",
    "\n",
    "```bash\n",
    "create_tfrecord /global/homes/x/xju/atlas/data/top-tagger/test.h5 tfrec/test \\\n",
    "  --evts-per-record 100 --max-evts 1000 \\\n",
    "  --type TopTaggerDataset --num-workers 2\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating graphs using networkx\n",
    "\n",
    "[networkx](https://networkx.org/documentation/stable/tutorial.html) is a Python package for the study of graphs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "from graph_nets import utils_np\n",
    "from graph_nets import utils_tf\n",
    "from graph_nets import graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.DiGraph()\n",
    "\n",
    "# add nodes\n",
    "[g.add_node(idx, features=np.array([1.*idx])) for idx in range(4)];\n",
    "\n",
    "# add edges\n",
    "edge_lists = [(0, 1), (1, 2), (2, 3), (3, 0)]\n",
    "[g.add_edge(i, j, features=np.array([abs(i-j)])) for i,j in edge_lists];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "pos = nx.spring_layout(g)\n",
    "nx.draw(g, pos, node_size=400, alpha=0.85, node_color=\"#1f78b4\", with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "obtain the adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = np.asarray(nx.to_numpy_matrix(g))\n",
    "adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_tuple = utils_np.networkxs_to_graphs_tuple([g])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_graphs_tuple(g, data=True):\n",
    "    for field_name in graphs.ALL_FIELDS:\n",
    "        per_replica_sample = getattr(g, field_name)\n",
    "        if per_replica_sample is None:\n",
    "            print(field_name, \"EMPTY\")\n",
    "        else:\n",
    "            print(field_name, \"is with shape\", per_replica_sample.shape)\n",
    "            if data and  field_name != \"edges\":\n",
    "                print(per_replica_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_graphs_tuple(g_tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create GraphsTuple using data-dict \\[recommend\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_node = 4\n",
    "n_node_features = 1\n",
    "n_edge = 4\n",
    "n_edge_features = 1\n",
    "nodes = np.random.rand(n_node, n_node_features).astype(np.float32)\n",
    "edges = np.random.rand(n_edge, n_edge_features).astype(np.float32)\n",
    "receivers = np.array([1, 2, 3, 0])\n",
    "senders = np.array([0, 1, 2, 3])\n",
    "datadict = {\n",
    "    \"n_node\": n_node,\n",
    "    \"n_edge\": n_edge,\n",
    "    \"nodes\": nodes,\n",
    "    \"edges\": edges,\n",
    "    \"senders\": senders,\n",
    "    \"receivers\": receivers,\n",
    "    \"globals\": np.array([0], dtype=np.float32)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_tuple2 = utils_tf.data_dicts_to_graphs_tuple([datadict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_graphs_tuple(g_tuple2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can you finish implementing the following function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected_edges(n_nodes: int):\n",
    "    \"\"\"For a given number of nodes, \n",
    "    return the senders and receivers for a fully-connected graph.\n",
    "    \"\"\"\n",
    "    \n",
    "    receivers = senders = n_edge = None\n",
    "    \n",
    "    return {\"receivers\": receivers, \"senders\": senders, \"n_edge\": n_edge}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert an event to a fully-connected graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/global/homes/a/ading/atlas/data/top-tagger/test.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.HDFStore(filename, mode='r') as store:\n",
    "    df = store['table']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['is_signal_new'] == 1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event = df.iloc[0]\n",
    "event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from typing import Optional\n",
    "\n",
    "features = ['E', 'PX', 'PY', 'PZ']\n",
    "scale = 0.001\n",
    "solution = 'is_signal_new'\n",
    "\n",
    "def make_graph(event, debug: Optional[bool] = False):\n",
    "    n_max_nodes = 200\n",
    "    n_nodes = 0\n",
    "    nodes = []\n",
    "    for inode in range(n_max_nodes):\n",
    "        E_name = 'E_{}'.format(inode)\n",
    "        if event[E_name] < 0.1:\n",
    "            continue\n",
    "\n",
    "        f_keynames = ['{}_{}'.format(x, inode) for x in features]\n",
    "        n_nodes += 1\n",
    "        nodes.append(event[f_keynames].values*scale)\n",
    "    nodes = np.array(nodes, dtype=np.float32)\n",
    "    # print(n_nodes, \"nodes\")\n",
    "    # print(\"node features:\", nodes.shape)\n",
    "\n",
    "    # edges 1) fully connected, 2) objects nearby in eta/phi are connected\n",
    "    # TODO: implement 2). <xju>\n",
    "    all_edges = list(itertools.combinations(range(n_nodes), 2))\n",
    "    senders = np.array([x[0] for x in all_edges])\n",
    "    receivers = np.array([x[1] for x in all_edges])\n",
    "    n_edges = len(all_edges)\n",
    "    edges = np.expand_dims(np.array([0.0]*n_edges, dtype=np.float32), axis=1)\n",
    "    # print(n_edges, \"edges\")\n",
    "    # print(\"senders:\", senders)\n",
    "    # print(\"receivers:\", receivers)\n",
    "\n",
    "    input_datadict = {\n",
    "        \"n_node\": n_nodes,\n",
    "        \"n_edge\": n_edges,\n",
    "        \"nodes\": nodes,\n",
    "        \"edges\": edges,\n",
    "        \"senders\": senders,\n",
    "        \"receivers\": receivers,\n",
    "        \"globals\": np.array([n_nodes], dtype=np.float32)\n",
    "    }\n",
    "    target_datadict = {\n",
    "        \"n_node\": n_nodes,\n",
    "        \"n_edge\": n_edges,\n",
    "        \"nodes\": nodes,\n",
    "        \"edges\": edges,\n",
    "        \"senders\": senders,\n",
    "        \"receivers\": receivers,\n",
    "        \"globals\": np.array([event[solution]], dtype=np.float32)\n",
    "    }\n",
    "    input_graph = utils_tf.data_dicts_to_graphs_tuple([input_datadict])\n",
    "    target_graph = utils_tf.data_dicts_to_graphs_tuple([target_datadict])\n",
    "    return [(input_graph, target_graph)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = make_graph(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_evt_input, g_evt_target = graphs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_graphs_tuple(g_evt_input, data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "17*16//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_evt_target.globals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The main script, train_classifier, can be invoked with the following bash command with default arguments:\n",
    "\n",
    "```bash\n",
    "train_classifier\n",
    "```\n",
    "\n",
    "or with the following arguments specifying I/O and hyperparameters:\n",
    "```bash\n",
    "train_classifier --input-dir tfrec --output-dir trained \\\n",
    "  --batch-size 25 --num-epochs 10 --num-iters 10 --lr 0.002\n",
    "```\n",
    "\n",
    "You can also specify other models and loss functions defined in ```root_gnn/model.py``` and ```root_gnn/losses.py```.\n",
    "\n",
    "Let's examine what ```train_classifier``` is doing under the hood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import functools\n",
    "import six\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "\n",
    "\n",
    "from graph_nets import utils_tf\n",
    "from graph_nets import utils_np\n",
    "import sonnet as snt\n",
    "\n",
    "from root_gnn import model as all_models\n",
    "from root_gnn import losses\n",
    "from root_gnn.src.datasets import graph\n",
    "from root_gnn.utils import load_yaml\n",
    "\n",
    "from root_gnn import trainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = \"../tfrec/train/*.tfrec\"\n",
    "\n",
    "\n",
    "model = getattr(all_models, \"GlobalClassifierNoEdgeInfo\")()\n",
    "loss_config = \"GlobalLoss,1,1\".split(',')\n",
    "loss_fcn = getattr(losses, loss_config[0])(*[float(x) for x in loss_config[1:]])\n",
    "config = {\n",
    "    \"input_dir\": \"../tfrec\",\n",
    "    \"output_dir\": \"../trained\",\n",
    "    \"batch_size\": 50,\n",
    "    \"num_epochs\": 5,\n",
    "    \"num_iters\": 10,\n",
    "    \"shuffle_size\": 1,\n",
    "    \"model\": model,\n",
    "    \"loss_name\": loss_config[0],\n",
    "    \"loss_fcn\": loss_fcn,\n",
    "    \"lr\": 0.001,\n",
    "    \"metric_mode\": \"clf\",\n",
    "    \"early_stop\": \"auc\",\n",
    "    \"max_attempts\": 1\n",
    "}\n",
    "trnr = trainer.TrainerBase(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```TrainerBase()``` constructor initializes a base trainer object by unpacking the ```config``` dict.\n",
    "\n",
    "Next, the user can call functions for loading training, validation, and testing data. The requirement is that the files to be extracted from```input_dir``` must be of the proper ```.tfrec``` format created by ```create_tfrecord```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: treat not as config, but as a standalone part\n",
    "\n",
    "train_data, _ = trnr.load_training_data(filenames=\"../tfrec/train*.tfrec\", shuffle=True)\n",
    "val_data, _ = trnr.load_validating_data(filenames=\"../tfrec/val*.tfrec\", shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```train``` function of ```TrainerBase``` performs training given the specified configurations and hyperparameters. The function can be called in two main ways:\n",
    "\n",
    "The first way to call ```train``` is by specifying either a model, loss, or training data. The training data must be of the format returned as the first tuple value of ```load_training_data```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trnr.train(model, loss_fcn, train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second way is the default call, which assumes the model and loss are the same as the configurations, and that the training data is the same as the last call to ```load_training_data```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trnr.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this next part, we will be using TensorBoard with the ```nersc_tensorboard_helper```. At this point, it is recommended to switch the notebook kernel away from \"a-Gnn\" to \"tensorflow-v2.0.0-cpu\" to access the tensorboard helper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nersc_tensorboard_helper\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir /global/homes/a/ading/root_gnn/trained/noedge_fullevts/logs --port 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nersc_tensorboard_helper.tb_address()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now access the link above to view the TensorBoard for your training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-v2.0.0-cpu",
   "language": "python",
   "name": "tensorflow_intel_2.0.0-py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
