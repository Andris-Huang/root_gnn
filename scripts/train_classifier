#!/usr/bin/env python
"""
Training GNN
"""

import tensorflow as tf

import os
import sys
import argparse

import re
import time
import random
import functools

import numpy as np
import sklearn.metrics


from graph_nets import utils_tf
from graph_nets import utils_np
import sonnet as snt

from root_gnn import model as all_models
from root_gnn import losses
# from root_gnn import optimizers
from heptrkx.dataset import graph
from heptrkx.utils import load_yaml

ckpt_name = 'checkpoint'

# prog_name = os.path.basename(sys.argv[0])

def eval_output(target, output):
    """
    target, output are graph-tuple from TF-GNN,
    each of them contains N=batch-size graphs
    """
    tdds = utils_np.graphs_tuple_to_data_dicts(target)
    odds = utils_np.graphs_tuple_to_data_dicts(output)

    test_target = []
    test_pred = []
    for td, od in zip(tdds, odds):
        test_target.append(td['globals'])
        test_pred.append(od['globals'])
        # test_target.append(np.squeeze(td['globals']))
        # test_pred.append(np.squeeze(od['globals']))

    test_target = np.concatenate(test_target, axis=0)
    test_pred   = np.concatenate(test_pred,   axis=0)
    return test_pred, test_target


def compute_matrics(target, output, thresh=0.5):
    test_pred, test_target = eval_output(target, output)
    y_pred, y_true = (test_pred > thresh), (test_target > thresh)
    return sklearn.metrics.precision_score(y_true, y_pred), sklearn.metrics.recall_score(y_true, y_pred)


def train_and_evaluate(args):

    output_dir = args.job_dir
    os.makedirs(output_dir, exist_ok=True)

    global_batch_size = args.train_batch_size
    num_processing_steps_tr = args.num_iters
    metric_name = args.early_stop_metric
    metric_dict = {
        "auc_te": 0.0, "acc_te": 0.0, "prec_te": 0.0, "rec_te": 0.0, "loss_te": 0.0
    }
    if metric_name not in metric_dict.keys():
        msg = "earlystop_metric: {} not supported. Use one of the following\n".format(metric_name) \
            + "\n".join(list(metric_dict.keys()))
        raise ValueError(msg)

    # prepare inputs
    tr_filenames = tf.io.gfile.glob(args.train_files)
    n_train = len(tr_filenames)
    val_filenames = tf.io.gfile.glob(args.eval_files)
    n_val = len(val_filenames)

    print("Input file names: ", tr_filenames)
    print("{} training files".format(n_train))
    print("{} evaluation files".format(n_val))
    print("save models at {}".format(output_dir))

    AUTO = tf.data.experimental.AUTOTUNE
    # options = tf.data.Options()
    # options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA
    # training_dataset = training_dataset.with_options(options)
    # training_dataset = training_dataset.batch(global_batch_size).prefetch(AUTO)

    training_dataset = tf.data.TFRecordDataset(tr_filenames)
    training_dataset = training_dataset.map(graph.parse_tfrec_function, num_parallel_calls=AUTO)

    if args.shuffle_buffer_size < 0:
        train_buffer_size = sum([1 for _ in training_dataset])
        eval_buffer_size  = sum([1 for _ in testing_dataset])
    else:
        train_buffer_size = eval_buffer_size = args.shuffle_buffer_size

    training_dataset = training_dataset.shuffle(train_buffer_size, seed=12345, reshuffle_each_iteration=False)
    training_dataset = training_dataset.prefetch(AUTO)

    testing_dataset = tf.data.TFRecordDataset(val_filenames)
    testing_dataset = testing_dataset.map(graph.parse_tfrec_function, num_parallel_calls=AUTO)
    testing_dataset = testing_dataset.shuffle(eval_buffer_size, seed=12345, reshuffle_each_iteration=False)
    testing_dataset = testing_dataset.prefetch(AUTO)

    learning_rate = args.learning_rate
    if args.optimizer_name == "Adam":
        optimizer = snt.optimizers.Adam(learning_rate)
    elif args.optimizer_name == "SGD":
        optimizer = snt.optimizers.SGD(learning_rate)
    elif args.optimizer_name == "CCA":
        # iterations = sum([1 for _ in training_dataset])
        pass
        # optimizer = optimizers.CCA(iterations, learning_rate)
    else:
        raise ValueError("SGD is not supported yet")

    # optimzer = tf.keras.optimizers.schedules.LearningRateSchedule()
    model = getattr(all_models, args.model_name)()

    # inputs, targets = doublet_graphs.create_graph(batch_size)
    with_batch_dim = False
    input_list = []
    target_list = []
    for dd in training_dataset.take(global_batch_size).as_numpy_iterator():
        input_list.append(dd[0])
        target_list.append(dd[1])

    inputs = utils_tf.concat(input_list, axis=0)
    targets = utils_tf.concat(target_list, axis=0)
    input_signature = (
        graph.specs_from_graphs_tuple(inputs, with_batch_dim),
        graph.specs_from_graphs_tuple(targets, with_batch_dim)
    )

    loss_name = args.loss_name
    loss_config = args.loss_args.split(',')
    loss_fcn = getattr(losses, loss_name)(*[float(x) for x in loss_config])

    @functools.partial(tf.function, input_signature=input_signature)
    def update_step(inputs_tr, targets_tr):
        print("Tracing update_step")
        with tf.GradientTape() as tape:
            outputs_tr = model(inputs_tr, num_processing_steps_tr)
            loss_ops_tr = loss_fcn(targets_tr, outputs_tr)
            loss_op_tr = tf.math.reduce_sum(loss_ops_tr) / tf.constant(num_processing_steps_tr, dtype=tf.float32)

        gradients = tape.gradient(loss_op_tr, model.trainable_variables)
        optimizer.apply(gradients, model.trainable_variables)
        return outputs_tr, loss_op_tr

    checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)
    ckpt_manager = tf.train.CheckpointManager(checkpoint, directory=output_dir, max_to_keep=5)
    if os.path.exists(os.path.join(output_dir, ckpt_name)):
        print("Loading latest checkpoint")
        status = checkpoint.restore(ckpt_manager.latest_checkpoint)

    logged_iterations = []
    losses_tr = []
    corrects_tr = []
    solveds_tr = []


    start_time = time.time()
    last_log_time = start_time
    ## loop over iterations, each iteration generating a batch of data for training
    iruns = 0
    # print("# (iteration number), TD (get graph), TR (TF run)")
    last_iteration = 0

    # log information
    out_str  = time.strftime('%d %b %Y %H:%M:%S', time.localtime())
    out_str += '\n'
    out_str += "# (iteration number), T (elapsed seconds), Ltr (training loss), Lge (testing loss)"\
        "AUC, Accuracy, Precision, Recall\n"
    log_name = os.path.join(output_dir, "log_training.txt")
    with open(log_name, 'a') as f:
        f.write(out_str)


    previous_metric = 0.0
    threshold = 0.5
    n_fails = 0

    for epoch in range(args.num_epochs):
        total_loss = 0.
        num_batches = 0

        in_list = []
        target_list = []
        for inputs in training_dataset:
            inputs_tr, targets_tr = inputs
            in_list.append(inputs_tr)
            target_list.append(targets_tr)
            if len(in_list) == global_batch_size:
                inputs_tr = utils_tf.concat(in_list, axis=0)
                targets_tr = utils_tf.concat(target_list, axis=0)
                total_loss += update_step(inputs_tr, targets_tr)[1].numpy()
                in_list = []
                target_list = []
                num_batches += 1

        ckpt_manager.save()

        eval_output_name = os.path.join(output_dir, "eval_{}.npz".format(
            ckpt_manager.checkpoint.save_counter.numpy()))
        last_log_time = time.time()
        loss_tr = total_loss/num_batches
        # print("{} batches".format(num_batches))

        elapsed = time.time() - start_time
        inputs_te_list = []
        target_te_list = []
        predictions = []
        truth_info = []
        num_batches_te = 0
        total_loss_te = 0
        for inputs in testing_dataset:
            inputs_te, targets_te = inputs
            inputs_te_list.append(inputs_te)
            target_te_list.append(targets_te)
            if len(inputs_te_list) == global_batch_size:
                inputs_te = utils_tf.concat(inputs_te_list, axis=0)
                targets_te = utils_tf.concat(target_te_list, axis=0)
                outputs_te = model(inputs_te, num_processing_steps_tr)
                total_loss_te += (tf.math.reduce_sum(
                    loss_fcn(targets_te, outputs_te))/tf.constant(
                        num_processing_steps_tr, dtype=tf.float32)).numpy()
                if loss_name == "GlobalLoss":
                    predictions.append(outputs_te[-1].globals)
                    truth_info.append(targets_te.globals)
                else:
                    predictions.append(outputs_te[-1].edges)
                    truth_info.append(targets_te.edges)
                inputs_te_list = []
                target_te_list = []
                num_batches_te += 1

        loss_te = total_loss_te / num_batches_te
        predictions = np.concatenate(predictions, axis=0)
        truth_info = np.concatenate(truth_info, axis=0)
        # print(tf.math.reduce_sum(predictions).numpy(), tf.math.reduce_sum(truth_info).numpy())

        y_true, y_pred = (truth_info > threshold), (predictions > threshold)
        fpr, tpr, _ = sklearn.metrics.roc_curve(y_true, predictions)
        metric_dict['auc_te'] = sklearn.metrics.auc(fpr, tpr)
        metric_dict['acc_te'] = sklearn.metrics.accuracy_score(y_true, y_pred)
        metric_dict['pre_te'] = sklearn.metrics.precision_score(y_true, y_pred)
        metric_dict['rec_te'] = sklearn.metrics.recall_score(y_true, y_pred)
        metric_dict['loss_te'] = loss_te
        out_str = "* {:05d}, T {:.1f}, Ltr {:.4f}, Lge {loss_te:.4f}, AUC {auc_te:.4f}, A {acc_te:.4f}, P {pre_te:.4f}, R {rec_te:.4f}".format(
            epoch, elapsed, loss_tr, **metric_dict)
        print(out_str)
        with open(log_name, 'a') as f:
            f.write(out_str+"\n")
        np.savez(eval_output_name, predictions=predictions, truth_info=truth_info)
        metric = metric_dict[metric_name]
        if metric < previous_metric:
            print("Current metric {} {:.4f} is lower than previous {:.4f}.".format(metric_name, metric, previous_metric))
            if n_fails < args.acceptable_fails:
                n_fails += 1
            else:
                print("Reached maximum failure threshold: {} times. Stop Training".format(args.acceptable_fails))
                break
        else:
            previous_metric = metric


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Train nx-graph with configurations')
    add_arg = parser.add_argument
    add_arg("--train-files", help="path to training data", required=True)
    add_arg("--eval-files", help='path to evaluation data', required=True)
    add_arg("--job-dir", help='location to write checkpoints and explort models', required=True)

    add_arg('--train-batch-size', help='batch size for training', default=1, type=int)
    add_arg("--eval-batch-size", help='batch size for evaluation', default=1, type=int)
    add_arg('--num-iters', help='number of message passing steps', default=8, type=int)
    add_arg("--num-epochs", help='number of epochs', default=1, type=int)
    add_arg('--optimizer-name', help='optimizer', default='Adam', choices=['Adam', 'SGD'])
    add_arg("--learning-rate", help='learning rate', default=0.0005, type=float)
    add_arg('--exp-decay-rate', help='exponential decay rate for learning rate, only appliable for SGD',\
        default=0.9, type=float)
    add_arg("--exp-decay-steps", help='exponential decay steps for learning rate, only applicable for SGD',\
        default=1000, type=int)
    add_arg('--cca', help='cyclic consine annealing for learning rate, only applicable for SGD,'\
        'the value indicates the number of epochs for each cycle',\
        default=-1, type=int)
    add_arg("--shuffle-buffer-size", help="shuffle buffer size", default=-1, type=int)

    add_arg("--model-name", help='model name',\
        default='GlobalClassifierNoEdgeInfo',
        choices=['GlobalClassifierNoEdgeInfo', 'GeneralClassifier', 'EdgeGlobalClassifier',
        'MultiClassifier', 'NodeEdgeClassifier', 'EdgeClassifier'])
    add_arg('--loss-name', help="Loss functions",\
        default='GlobalLoss', 
        choices=['NodeEdgeLoss', 'GlobalLoss', 'EdgeGlobalLoss', 'EdgeLoss'])
    add_arg("--loss-args",\
        help='arguments passed to the loss function; they are the weights for signal and background', 
        default='1, 1')

    add_arg("--early-stop-metric", help='metric used for early stop', default='auc_te',
        choices=['auc_te', 'acc_te', 'pre_te', 'rec_te'])
    add_arg("--acceptable-fails", help='number of failures allowed', default=0, type=int)

    args = parser.parse_args()

    train_and_evaluate(args)