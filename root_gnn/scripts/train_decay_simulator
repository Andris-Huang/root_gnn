#!/usr/bin/env python

import tensorflow as tf
# import tensorflow.experimental.numpy as tnp

import os
import sys
import argparse

import re
import time
import random
import functools
import six

import numpy as np
import sklearn.metrics


from graph_nets import utils_tf
from graph_nets import utils_np
import sonnet as snt

from types import SimpleNamespace
import tensorflow as tf
from tensorflow.compat.v1 import logging
logging.info("TF Version:{}".format(tf.__version__))

from scipy.sparse import coo_matrix

from root_gnn import model as all_models
from root_gnn.src.datasets import herwig_hadrons_v2
from root_gnn.src.datasets import graph
from root_gnn.utils import load_yaml


from root_gnn.trainer import read_dataset
from root_gnn.trainer import loop_dataset
from root_gnn.trainer import get_signature

MAX_PREV_NODES = 15
MAX_NODES = 84 

def init_workers(distributed=False):
    return SimpleNamespace(rank=0, size=1, local_rank=0, local_size=1, comm=None)
        

# def encode_adj(adj, max_prev_node=15, is_full=False):
#     '''

#     :param adj: n*n, rows means time step, while columns are input dimension
#     :param max_prev_node: we want to keep row number, but truncate column numbers
#     :return:
#     '''
#     if is_full:
#         max_prev_node = adj.shape[0]-1

#     # pick up lower tri
#     adj = np.tril(adj, k=-1)
#     n = adj.shape[0]
#     adj = adj[1:n, 0:n-1]

#     # use max_prev_node to truncate
#     # note: now adj is a (n-1)*(n-1) matrix
#     adj_output = np.zeros((adj.shape[0], max_prev_node))
#     for i in range(adj.shape[0]):
#         input_start = max(0, i - max_prev_node + 1)
#         input_end = i + 1
#         output_start = max_prev_node + input_start - input_end
#         output_end = max_prev_node
#         adj_output[i, output_start:output_end] = adj[i, input_start:input_end]
#         adj_output[i,:] = adj_output[i,:][::-1] # reverse order

#     return adj_output


def decode_adj(adj_output):
    '''
        recover to adj from adj_output
        note: here adj_output have shape (n-1)*m
    '''
    max_prev_node = adj_output.shape[1]
    adj = np.zeros((adj_output.shape[0], adj_output.shape[0]))
    for i in range(adj_output.shape[0]):
        input_start = max(0, i - max_prev_node + 1)
        input_end = i + 1
        output_start = max_prev_node + max(0, i - max_prev_node + 1) - (i + 1)
        output_end = max_prev_node
        adj[i, input_start:input_end] = adj_output[i,::-1][output_start:output_end] # reverse order
    adj_full = np.zeros((adj_output.shape[0]+1, adj_output.shape[0]+1))
    n = adj_full.shape[0]
    adj_full[1:n, 0:n-1] = np.tril(adj, 0)
    adj_full = adj_full + adj_full.T

    return adj_full

# def tf_decode_adj(adj_output):
#     '''
#         recover to adj from adj_output
#         note: here adj_output have shape (n-1)*m
#     '''
#     max_prev_node = adj_output.shape[1]
#     adj = tnp.zeros((adj_output.shape[0], adj_output.shape[0]))
#     for i in range(adj_output.shape[0]):
#         input_start = max(0, i - max_prev_node + 1)
#         input_end = i + 1
#         output_start = max_prev_node + max(0, i - max_prev_node + 1) - (i + 1)
#         output_end = max_prev_node
#         adj[i, input_start:input_end] = adj_output[i,::-1][output_start:output_end] # reverse order
#     adj_full = tnp.zeros((adj_output.shape[0]+1, adj_output.shape[0]+1))
#     n = adj_full.shape[0]
#     adj_full[1:n, 0:n-1] = np.tril(adj, 0)
#     adj_full = adj_full + adj_full.T
#     return adj_full

def decode_adj_to_edges(adj_output):
    senders = []
    receivers = []
    
    max_prev_node = adj_output.shape[1]
    
    zero = tf.constant(0, dtype=tf.int32)
    for i in range(adj_output.shape[0]):
        input_start = max(0, i - max_prev_node + 1)
        input_end = i + 1
        output_start = max_prev_node + max(0, i - max_prev_node + 1) - (i + 1)
        output_end = max_prev_node
        
        where, = np.nonzero(adj_output[i,::-1][output_start:output_end])
        # where = tf.where(tf.math.not_equal(adj_output[i,::-1][output_start:output_end], zero))
        # print(where.shape)
        if where.shape[0] > 0:
            for idx in range(where.shape[0]):
                receivers.append(i+1)
                senders.append(where[idx]+input_start)
            # for idx in range(where.shape[0]):
            #     receivers.append(i+1)
            #     senders.append(where[idx, 0].numpy()+input_start)

    if len(senders) > 0:
        all_senders = senders + receivers
        all_receivers = receivers + senders
    else:
        all_senders = [0]
        all_receivers = [0]

    return (tf.constant(all_senders), tf.constant(all_receivers))

def hacky_sigmoid_l2(nodes):
    r = tf.reduce_sum(tf.square(nodes), 1)
    r = tf.reshape(r, [-1, 1])
    D = r - 2 * tf.matmul(nodes, nodes, transpose_b=True) + tf.transpose(r)
    #TODO: make tunable param
    r = 10
    return tf.math.sigmoid(r * (1 - D))

def scaled_hacky_sigmoid_l2(nodes):
    dim = tf.shape(nodes)[1]
    r = tf.reduce_sum(tf.square(nodes), 1)
    r = tf.reshape(r, [-1, 1])
    D = r - 2 * tf.matmul(nodes, nodes, transpose_b=True) + tf.transpose(r)
    #TODO: make tunable param
    D /= tf.sqrt(tf.dtypes.cast(dim, tf.float32))
    r = 10
    return tf.math.sigmoid(r * (1 - D))

# def adj_to_edges(adj):
#     return tnp.nonzero(adj)

def edges_to_adj(senders, receivers, n_nodes):
    n_edges = receivers.shape[0]
    coo = coo_matrix((np.ones((n_edges, )),
                  (senders, receivers)), shape=(n_nodes, n_nodes))
    return coo.toarray()

def train_and_evaluate(args):
    dist = init_workers(args.distributed)

    device = 'CPU'
    gpus = tf.config.experimental.list_physical_devices("GPU")
    logging.info("found {} GPUs".format(len(gpus)))

    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)

    if len(gpus) > 0:
        device = "{}GPUs".format(len(gpus))
    if gpus and args.distributed:
        tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')

    time_stamp = time.strftime('%Y%m%d-%H%M%S', time.localtime())
    output_dir = args.output_dir
    # output_dir = os.path.join(args.output_dir, "{}".format(time_stamp))
    if dist.rank == 0:
        os.makedirs(output_dir, exist_ok=True)
    logging.info("Checkpoints and models saved at {}".format(output_dir))

    n_epochs = args.max_epochs
    logging.info("{} epochs with batch size {}".format(n_epochs, args.batch_size))
    logging.info("I am in hvd rank: {} of  total {} ranks".format(dist.rank, dist.size))

    if dist.rank == 0:
        train_input_dir = os.path.join(args.input_dir, 'train') 
        val_input_dir = os.path.join(args.input_dir, 'val')
        train_files = tf.io.gfile.glob(os.path.join(train_input_dir, args.patterns))
        eval_files = tf.io.gfile.glob(os.path.join(val_input_dir, args.patterns))
        ## split the number of files evenly to all ranks
        train_files = [x.tolist() for x in np.array_split(train_files, dist.size)]
        eval_files = [x.tolist() for x in np.array_split(eval_files, dist.size)]
    else:
        train_files = None
        eval_files = None

    if args.distributed:
        train_files = dist.comm.scatter(train_files, root=0)
        eval_files = dist.comm.scatter(eval_files, root=0)
    else:
        train_files = train_files[0]
        eval_files = eval_files[0]

    logging.info("rank {} has {} training files and {} evaluation files".format(
        dist.rank, len(train_files), len(eval_files)))

    AUTO = tf.data.experimental.AUTOTUNE
    training_dataset, ngraphs_train = read_dataset(train_files)
    training_dataset = training_dataset.prefetch(AUTO)
    logging.info("rank {} has {} training graphs".format(dist.rank, ngraphs_train))

    input_signature = get_signature(training_dataset, args.batch_size)


    learning_rate = args.learning_rate
    optimizer_gen = snt.optimizers.Adam(learning_rate)
    optimizer_disc = snt.optimizers.Adam(learning_rate)
    model_gen = getattr(all_models, 'GraphGenerator')(
        max_prev_nodes=MAX_PREV_NODES, noise_dimension=args.noise_dim)
    model_disc = getattr(all_models, 'GlobalClassifierNoEdgeInfo')()


    def generator_loss(fake_output):
        loss_ops = [tf.compat.v1.losses.log_loss(tf.ones_like(output_op.globals, dtype=tf.float32), output_op.globals)
                    for output_op in fake_output
                ]
        return tf.stack(loss_ops)

    def discriminator_loss(real_output, fake_output):
        loss_ops = [tf.compat.v1.losses.log_loss(tf.ones_like(output_op.globals, dtype=tf.float32), output_op.globals)
                for output_op in real_output]
        loss_ops += [tf.compat.v1.losses.log_loss(tf.zeros_like(output_op.globals, dtype=tf.float32), output_op.globals)
                for output_op in fake_output]
        return tf.stack(loss_ops)


    # @functools.partial(tf.function, input_signature=input_signature)
    def train_step(inputs_tr, targets_tr, first_batch):
        noise = tf.random.normal([args.batch_size, args.noise_dim])
        input_op = tf.concat([inputs_tr.nodes, noise], axis=-1)

        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
            # generator, and prepare the predicted graph
            n_nodes = tf.math.reduce_sum(targets_tr.n_node)
            # print("total nodes: ", n_nodes)
            encoded_adj, node_pred = model_gen(input_op, max_nodes=MAX_NODES, training=True)

            encoded_adj = tf.math.sigmoid(tf.stack(encoded_adj, axis=0)) > 0.5
            encoded_adj = tf.cast(encoded_adj, dtype=tf.int32)
            encoded_adj = tf.transpose(encoded_adj, perm=[1, 0, 2]) # cannot use reshape

            node_pred = tf.transpose(tf.stack(node_pred, axis=0), perm=[1, 0, 2])

            # adj = decode_adj(encoded_adj)
            # predicted_edges = adj_to_edges(adj)
            predicted_graphs = []
            for ibatch in range(args.batch_size):
                enc_adj = encoded_adj[ibatch]
                nodes = tf.concat([inputs_tr.nodes[ibatch:ibatch+1, ], node_pred[ibatch]], axis=0)
                senders, receivers = decode_adj_to_edges(enc_adj)
                # print("ibatch: ", ibatch)
                # print("nodes: ", nodes.shape)
                # print("senders: ", senders.shape)
                # print("receivers: ", receivers.shape)
                n_edge = senders.shape[0]
                # if n_edge == 0:
                #     senders = receivers = np.array([0], dtype=np.int32)
                #     n_edge = 1

                pred_graph = utils_tf.data_dicts_to_graphs_tuple([
                    {
                        "n_node": MAX_NODES, 
                        "n_edge": n_edge,
                        "nodes": nodes,
                        # "nodes": node_pred[node_removal], 
                        "edges": np.expand_dims(np.array([0.0]*n_edge, dtype=np.float32), axis=1),
                        "senders": senders,
                        "receivers": receivers,
                        "globals": np.array([0], dtype=np.float32)
                    }
                ])
                predicted_graphs.append(pred_graph)
            
            predicted_graphs = utils_tf.concat(predicted_graphs, axis=0)
            # discriminator
            real_output = model_disc(targets_tr, args.dis_num_iters)
            fake_output = model_disc(predicted_graphs, args.dis_num_iters)

            gen_loss = generator_loss(fake_output)
            disc_loss = discriminator_loss(real_output, fake_output)

            gen_loss = tf.math.reduce_sum(gen_loss) / tf.constant(args.dis_num_iters, dtype=tf.float32)
            disc_loss = tf.math.reduce_sum(disc_loss) / tf.constant(args.dis_num_iters, dtype=tf.float32)

        gradients_of_generator = gen_tape.gradient(gen_loss, model_gen.trainable_variables)
        gradients_of_discriminator = disc_tape.gradient(disc_loss, model_disc.trainable_variables)

        optimizer_gen.apply(gradients_of_generator, model_gen.trainable_variables)
        optimizer_disc.apply(gradients_of_discriminator, model_disc.trainable_variables)

        return gen_loss, disc_loss

    def train_epoch(dataset):
        total_gen_loss = 0
        total_disc_loss = 0
        batch = 0
        for inputs in loop_dataset(dataset, args.batch_size):
            inputs_tr, targets_tr = inputs
            gen_loss, disc_loss = train_step(inputs_tr, targets_tr, batch==0)
            batch += 1
            total_gen_loss += gen_loss.numpy()
            total_disc_loss += disc_loss.numpy()
        return total_gen_loss/batch, total_disc_loss/batch, batch

    
    log_dir = os.path.join(output_dir, "logs/{}/train".format(time_stamp))
    train_summary_writer = tf.summary.create_file_writer(log_dir)

    ckpt_dir = os.path.join(output_dir, "checkpoints")
    checkpoint = tf.train.Checkpoint(
        optimizer_gen=optimizer_gen,
        optimizer_disc=optimizer_disc,
        model_gen=model_gen, model_disc=model_disc)

    ckpt_manager = tf.train.CheckpointManager(checkpoint, directory=ckpt_dir,
                                max_to_keep=5, keep_checkpoint_every_n_hours=8)
    logging.info("Loading latest checkpoint from: {}".format(ckpt_dir))
    _ = checkpoint.restore(ckpt_manager.latest_checkpoint)


    start_time = time.time()
    for epoch in range(n_epochs):

        training_dataset = training_dataset.shuffle(args.shuffle_size, seed=12345, reshuffle_each_iteration=True)

        gen_loss, disc_loss, batches = train_epoch(training_dataset)
        this_epoch = time.time()

        logging.info("{} epoch takes {:.2f} mins with generator loss {:.4f}, discriminator loss {:.4f} in {} batches".format(
            epoch, (this_epoch-start_time)/60., gen_loss, disc_loss, batches))

        ckpt_manager.save()
        # log some metrics
        with train_summary_writer.as_default():
            tf.summary.scalar("generator loss", gen_loss, step=epoch)
            tf.summary.scalar("discriminator loss", disc_loss, step=epoch)
            tf.summary.scalar("time", (this_epoch-start_time)/60., step=epoch)

        # print(inputs_tr.nodes.shape)
        # n_nodes = tf.math.reduce_sum(targets_tr.n_node)

        # noise = tf.random.normal([1, args.noise_dim])
        # input_op = tf.concat([inputs_tr.nodes, noise], axis=1)
        # encoded_adj, node_pred = model_gen(input_op, is_training=True)
        # # print(encoded_adj)
        # # print(node_pred)
        # node_pred = tf.concat([inputs_tr.nodes[0:1, ]]+node_pred, axis=0)
        # print("predicted node properties:", node_pred.shape)
        # encoded_adj = tf.math.sigmoid(tf.concat(encoded_adj, axis=0)) > 0.5
        # encoded_adj = tf.cast(encoded_adj, dtype=tf.int32)
        # print("encoded adj: ", encoded_adj.shape)

        # adj = decode_adj(encoded_adj.numpy())
        # print("decoded adj: ", adj.shape)
        # node_removal = ~tf.math.reduce_all(adj, axis=1)
        # adj = adj[~np.all(adj == 0, axis=1)]
        # adj = adj[:, ~tf.math.reduce_all(adj, axis=0)]
        # print("after removeal: ", adj.shape)

        # predicted_edges = adj_to_edges(adj)
        # n_pred_nodes = adj.shape[0]
        # n_pred_edges = predicted_edges[0].shape[0]
        # predicted_graph = utils_tf.data_dicts_to_graphs_tuple([
        #     {
        #         "n_node": n_pred_nodes, 
        #         "n_edge": n_pred_edges,
        #         "nodes": node_pred[node_removal], 
        #         "edges": np.expand_dims(np.array([0.0]*n_pred_edges, dtype=np.float32), axis=1),
        #         "senders": predicted_edges[0],
        #         "receivers": predicted_edges[1], 
        #         "globals": np.array([0], dtype=np.float32)
        #     }
        # ])
        # print(predicted_graph)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Train nx-graph with configurations')
    add_arg = parser.add_argument
    add_arg("--input-dir", help='input directory that contains subfolder of train, val and test')
    add_arg("--patterns", help='file patterns', default='*')
    add_arg("--output-dir", help="where the model and training info saved")
    add_arg('-d', '--distributed', action='store_true', help='data distributed training')
    add_arg("--learning-rate", help='learing rate', default=0.0005, type=float)
    add_arg("--max-epochs", help='number of epochs', default=1, type=int)
    add_arg("--batch-size", type=int, help='training/evaluation batch size', default=500)
    add_arg("--shuffle-size", type=int, help="number of events for shuffling", default=650)

    add_arg("--noise-dim", type=int, help='dimension of noises', default=8)
    add_arg("--dis-num-iters", type=int, help='number of message passing', default=4)

    add_arg("-v", "--verbose", help='verbosity', choices=['DEBUG', 'ERROR', 'FATAL', 'INFO', 'WARN'],\
        default="INFO")
    args, _ = parser.parse_known_args()

    # Set python level verbosity
    logging.set_verbosity(args.verbose)
    # Suppress C++ level warnings.
    # os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
    train_and_evaluate(args)